{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loaded dataset and normalized\n"
     ]
    }
   ],
   "source": [
    "# Load in the training set\n",
    "X_train = np.load('Assignment1-Dataset/train_data.npy')\n",
    "y_train = np.load('Assignment1-Dataset/train_label.npy')\n",
    "# Reshape the label to a 1-d vector\n",
    "y_train = y_train.reshape(-1,)\n",
    "\n",
    "# # Load in the test set\n",
    "X_test = np.load('Assignment1-Dataset/test_data.npy')\n",
    "y_test = np.load('Assignment1-Dataset/test_label.npy')\n",
    "# Reshape the label to a 1-d vector\n",
    "y_test = y_test.reshape(-1,)\n",
    "\n",
    "# Normalize (optional)\n",
    "def normalize(X, X2):\n",
    "    X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "    X2 = (X2 - X.mean(axis=0)) / X.std(axis=0) # Normalize the data with the mean and sd from train set\n",
    "    return X, X2\n",
    "\n",
    "X_train, X_test = normalize(X_train, X_test)\n",
    "\n",
    "print(\"Loaded dataset and normalized\")"
   ]
  },
  {
   "source": [
    "## Data Exploratory"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "There are 50000 rows and 128 features.\n",
      "\n",
      "There is 0 missing value.\n",
      "\n",
      "The largest and smallest range of a feature is 21.29 and 6.59.\n",
      "\n",
      "Average proportion of outliers is 2.59%.\n",
      "Max proportion of outliers is 5.42%.\n",
      "Since the dataset has 50000 data points, we agree that the outliers won't impose great effect\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(X_train)\n",
    "\n",
    "print('There are {} rows and {} features.'.format(X_train.shape[0], X_train.shape[1]))\n",
    "print()\n",
    "# Check if there is missing value\n",
    "df_na = df.isna().sum()\n",
    "print(\"There is {} missing value.\".format(len(df_na[df_na > 0])))\n",
    "print()\n",
    "\n",
    "df_ranged = df.max(axis=0) - df.min(axis=0)\n",
    "print(\"The largest and smallest range of a feature is {:.2f} and {:.2f}.\".format(df_ranged.max(), df_ranged.min()))\n",
    "print()\n",
    "\n",
    "# Function to return the upper and lower boundary\n",
    "# for not being a outlier\n",
    "def iqr(feature):\n",
    "    q75, q25 = np.percentile(feature, [75 ,25])\n",
    "    qr = q75 - q25\n",
    "    upper = q75 + (1.5 * qr)\n",
    "    lower = q25 - (1.5 * qr)\n",
    "    return upper, lower\n",
    "\n",
    "# Function to detect outlier\n",
    "def outlier(i, upper, lower):\n",
    "    if i > upper: return True\n",
    "    elif i < lower: return True\n",
    "    else: return False\n",
    "\n",
    "# Dict to see how many outlier are there (%) in each feature\n",
    "outlier_count = {}\n",
    "for i in df.columns:\n",
    "    up, low = iqr(df[i])\n",
    "    detection = df[i].apply(lambda x: outlier(x, up, low))\n",
    "    outlier_count[i] = detection.sum() * 100 / detection.count()\n",
    "\n",
    "\n",
    "print(\"Average proportion of outliers is {:.2f}%.\".format(np.array(list(outlier_count.values())).mean()))\n",
    "print(\"Max proportion of outliers is {:.2f}%.\".format(np.array(list(outlier_count.values())).max()))\n",
    "print(\"Since the dataset has {} data points, we agree that the outliers won't impose great effect\".format(df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Dense Layer class\n",
    "class dense:\n",
    "    # Initiate the class with required input and output dimension, and regularization\n",
    "    def __init__(self, input_dim, output_dim, w_regularizer_l1=0, w_regularizer_l2=0, \n",
    "b_regularizer_l1=0, b_regularizer_l2=0):\n",
    "        \n",
    "        # Initiate the weights and bias\n",
    "        self.w = 0.01 * np.random.randn(input_dim, output_dim)\n",
    "        self.b = np.zeros((1, output_dim))\n",
    "        \n",
    "        # Initiate the regularizer lambda value\n",
    "        self.w_regularizer_l1 = w_regularizer_l1\n",
    "        self.w_regularizer_l2 = w_regularizer_l2\n",
    "        self.b_regularizer_l1 = b_regularizer_l1\n",
    "        self.b_regularizer_l2 = b_regularizer_l2\n",
    "    \n",
    "    # Forward propagation of the Dense layer\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs # Save the inputs when it is pased into the layer\n",
    "        self.output = inputs.dot(self.w) + self.b # Calculate the output of this later\n",
    "    \n",
    "    # Backward propagation of the Dense layer\n",
    "    def backward(self, dvalues):\n",
    "        # Get gradient on weights and bias\n",
    "        # Dot product of input transposed and the gradient from previous layer\n",
    "        self.d_w = np.dot(self.inputs.T, dvalues)\n",
    "        self.d_b = np.sum(dvalues, axis=0, keepdims=True) # Sum up the delta from previous layer\n",
    "        \n",
    "        # Add the regularization penalties calculated from the original weights if to the gradient of the weights\n",
    "        if self.w_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.w)\n",
    "            # For negative weights, the derivative of it is -1\n",
    "            dL1[self.w < 0] = -1\n",
    "            self.d_w += self.w_regularizer_l1 * dL1\n",
    "        \n",
    "        if self.w_regularizer_l2 > 0:\n",
    "            self.d_w += 2 * self.w_regularizer_l1 * self.w\n",
    "\n",
    "        # Add the regularization penalties calculated from the original bias if to the gradient of the weights\n",
    "        if self.b_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.b)\n",
    "            # For negative bias, the derivative of it is -1\n",
    "            dL1[self.b < 0] = -1\n",
    "            self.d_w += self.b_regularizer_l1 * dL1\n",
    "        \n",
    "        if self.b_regularizer_l2 > 0:\n",
    "            self.d_b += self.b_regularizer_l2 * self.b\n",
    "            \n",
    "        # Gradient wrt inputs to be backprogated to the next layer\n",
    "        self.dinputs = np.dot(dvalues, self.w.T)\n",
    "    \n",
    "# Create the ReLu activaton layer class\n",
    "class ReLu:\n",
    "    \n",
    "    # Forward propagation for ReLu\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs # Save the inputs passed into this later\n",
    "        self.output = np.maximum(0, inputs) # Apply the relu function on each inputs\n",
    "    \n",
    "    # Back propagation for Relu\n",
    "    def backward(self, dvalues):\n",
    "        # Use the copy of the gradient instead on changing them\n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[self.inputs <0] = 0 # Apply the derivative of ReLu to the gradient wrt to gradient from last layer\n",
    "\n",
    "# Create the Softmax layer class\n",
    "class softmax:\n",
    "    # Forward propagation for Softmax\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs # Save the inputs passed into this layer\n",
    "        \n",
    "        # Apply the softmax function to the input\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        prob = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        \n",
    "        # Save the output values (predicted probabilities of each class) in this layer\n",
    "        self.output = prob\n",
    "    \n",
    "    # Back propagation for Softmax\n",
    "    def backward(self, dvalues):\n",
    "        # Create an empty array with shape (n_class,)\n",
    "        self.inputs = np.empty_like(dvalues)\n",
    "        \n",
    "        # Calculating the gradient of Softmax layer using its derivative\n",
    "        for idx, (single_output, single_dvalue) in enumerate(zip(self.output, dvalues)):\n",
    "            single_output = single_output.reshape(-1, 1)\n",
    "            jacob_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "        self.dinputs[idx] = np.dot(jacob_matrix, single_dvalue)\n",
    "    \n",
    "# Create a general Loss class for different loss function\n",
    "class Loss:\n",
    "    \n",
    "    # Add l1, l2, loss to a lump sum regularization loss\n",
    "    def regularization_loss(self, layer):\n",
    "        regularization_loss = 0\n",
    "        # Add l1 loss of weights to regularization loss\n",
    "        if layer.w_regularizer_l1 > 0:\n",
    "            regularization_loss += layer.w_regularizer_l1 * np.sum(np.abs(layer.w))\n",
    "        # Add l2 loss of weights to regularization loss\n",
    "        if layer.w_regularizer_l2 > 0:\n",
    "            regularization_loss += layer.w_regularizer_l2 * np.sum(np.abs(layer.w * layer.w))\n",
    "        # Add l1 loss of biasa to regularization loss \n",
    "        if layer.b_regularizer_l1 > 0:\n",
    "            regularization_loss += layer.b_regularizer_l1 * np.sum(np.abs(layer.b))\n",
    "        # Add l2 loss of biasa to regularization loss\n",
    "        if layer.b_regularizer_l2 > 0:\n",
    "            regularization_loss += layer.b_regularizer_l1 * np.sum(np.abs(layer.b * layer.b))\n",
    "        \n",
    "        return regularization_loss\n",
    "    \n",
    "    # A vanilla way to calculate loss using the loss function without regularization\n",
    "    def calculate(self, output, y):\n",
    "        sample_losses = self.forward(output, y)\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        return data_loss\n",
    "    \n",
    "# Create Crossentropy loss class with inherit the generl Loss class\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "    \n",
    "    # Forward propagation of CrossEntropy loss function\n",
    "    # which takes the output of the softmax layer as y_pred\n",
    "    def forward(self, y_pred, y):\n",
    "        sample = len(y_pred)\n",
    "        \n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7) # clipped the prediction to avoid divisio by 0\n",
    "        \n",
    "        if len(y.shape) == 1:\n",
    "            # Produce the vector of the predicted probability of the correct label\n",
    "            correct_confidence = y_pred_clipped[range(sample), y] \n",
    "        if len(y.shape) == 2:\n",
    "            correct_confidence = np.sum(y_pred_clipped * y, axis=1)\n",
    "        \n",
    "        # Calculate the negative log likelihoods\n",
    "        negative_log_likelihoods = -np.log(correct_confidence)\n",
    "        return negative_log_likelihoods\n",
    "    \n",
    "    # Back propagation of CrossEntropy loss function\n",
    "    def backward(self, dvalues, y):\n",
    "        \n",
    "        # Get the number of predictions (length of y_pred)\n",
    "        sample = len(dvalues)\n",
    "        # Get the number of class in the prediction\n",
    "        labels = len(dvalues[0])\n",
    "        \n",
    "        # Create one-hot encoding if y is an index label\n",
    "        if len(y.shape) == 1:\n",
    "            y = np.eye(labels)[y]\n",
    "        \n",
    "        # Calculate the gradient wrt to input values using the derivative of the crossentropy loss\n",
    "        self.dinputs = -y / dvalues\n",
    "        \n",
    "        # Normalize the gradient according to the batch size\n",
    "        self.dinputs = self.dinputs / sample\n",
    "\n",
    "# Create the output layer as a combination of Softmax and CrossEntroly Loss\n",
    "class softmax_crossentropy_loss():\n",
    "    \n",
    "    # Initiate the layer with the chosen activation and loss function\n",
    "    def __init__(self):\n",
    "        self.activation = softmax()\n",
    "        self.loss = Loss_CategoricalCrossentropy()\n",
    "    \n",
    "    # Forward propogation for this layer\n",
    "    def forward(self, inputs, y, test=False):\n",
    "            \n",
    "            # Get predictsion by pass forwad in the Softmax class\n",
    "            # with the input it received from the previous layer\n",
    "            self.activation.forward(inputs)\n",
    "            \n",
    "            # Save the output values (prediction) in this layer\n",
    "            self.output = self.activation.output\n",
    "            \n",
    "            # If we are doing testing, no more follow up action\n",
    "            if test == True:\n",
    "                pass\n",
    "            # During training, use the chosen loss function to get the loss\n",
    "            elif test == False:\n",
    "                return self.loss.calculate(self.output, y)\n",
    "    \n",
    "    # Back propagation for this layer\n",
    "    def backward(self, dvalues, y):\n",
    "        sample = len(dvalues)\n",
    "        \n",
    "        # In case the predicted values are vector\n",
    "        # Get the index with highest probability\n",
    "        if len(y) == 2:\n",
    "            y = np.argmax(y, axis=1)\n",
    "        \n",
    "        # Calculate the gradient of from the cross entropy loss\n",
    "        # dinputs and dvalues here are the output (prediction) from the output layer (softmax)\n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[range(sample), y] -= 1 # Calculate the gradient of loss and softmax using chain rule\n",
    "        self.dinputs = self.dinputs / sample # Normalize the gradient\n",
    "\n",
    "# Create the Optimizer (Stochastic gradient descent) class\n",
    "class Optimizer_SGD:\n",
    "    \n",
    "    # Initiate the class with the learning rate, decay rate and momentum\n",
    "    def __init__(self, lr=0.1, decay=0., momentum=0.):\n",
    "        self.lr = lr\n",
    "        self.current_lr = lr\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.momentum = momentum\n",
    "    \n",
    "    # Precall the decay function\n",
    "    # Since interation initiated at 0, the learning rate is still the preset one\n",
    "    def pre_update(self):\n",
    "        # Continue update the learning rate with the decay rate after each epochs\n",
    "        if self.decay:\n",
    "            self.current_lr = self.lr * (1 / (1 + self.decay * self.iterations))\n",
    "    \n",
    "    # Update the weights and bias with the gradient from back propagation\n",
    "    def update_params(self, layer):\n",
    "        \n",
    "        # If there is a momentum, we apply momentun to the updates\n",
    "        if self.momentum:\n",
    "            if not hasattr(layer, 'weight_momentums'):\n",
    "                \n",
    "                # Momentum at start is 0\n",
    "                layer.weight_momentums = np.zeros_like(layer.w)\n",
    "                layer.bias_momentum = np.zeros_like(layer.b)\n",
    "            \n",
    "            # The negative update is increased by the momentum\n",
    "            w_updates = self.momentum * layer.weight_momentums - self.current_lr * layer.d_w\n",
    "            \n",
    "            # Update the momentum from the update after each epoch\n",
    "            layer.weight_momentums = w_updates\n",
    "            \n",
    "            # Similar handling for bias\n",
    "            b_updates = self.momentum * layer.bias_momentum - self.current_lr * layer.d_b\n",
    "            layer.bias_momentum = b_updates\n",
    "        \n",
    "        else:\n",
    "            # Regular updates by multiplying the gradient with the learning rate\n",
    "            w_updates -= self.current_lr * layer.d_w\n",
    "            b_updates -= self.current_lr * layer.d_b\n",
    "        \n",
    "        # Update the weights and bias\n",
    "        layer.w += w_updates\n",
    "        layer.b += b_updates\n",
    "        \n",
    "    def update_BN_param(self, layer):\n",
    "        \n",
    "        # If there is a momentum, we apply momentun to the updates\n",
    "        if self.momentum:\n",
    "            if not hasattr(layer, 'gamma_momentums'):\n",
    "                \n",
    "                # Momentum at start is 0\n",
    "                layer.gamma_momentums = np.zeros_like(layer.gamma)\n",
    "                layer.beta_momentum = np.zeros_like(layer.beta)\n",
    "            \n",
    "            # The negative update is increased by the momentum\n",
    "            gamma_updates = self.momentum * layer.gamma_momentums - self.current_lr * layer.d_gamma\n",
    "            \n",
    "            # Update the momentum from the update after each epoch\n",
    "            layer.gamma_momentums = gamma_updates\n",
    "            \n",
    "            # Similar handling for bias\n",
    "            beta_updates = self.momentum * layer.beta_momentum - self.current_lr * layer.d_beta\n",
    "            layer.beta_momentum = beta_updates\n",
    "        \n",
    "        else:\n",
    "            # Regular updates by multiplying the gradient with the learning rate\n",
    "            gamma_updates -= self.current_lr * layer.d_gamma\n",
    "            beta_updates -= self.current_lr * layer.d_beta\n",
    "        \n",
    "        # Update the weights and bias\n",
    "        layer.gamma += gamma_updates\n",
    "        layer.beta += beta_updates\n",
    "    \n",
    "    # Keep track of the number of epoches\n",
    "    def post_update(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "# Create the Dropout layer\n",
    "class Dropout:\n",
    "    # Initiate layer with drop out rate (drop out here is the ratio of units to be disabled)\n",
    "    def __init__(self, rate):\n",
    "        self.rate = 1-rate\n",
    "    \n",
    "    # Forward propagation for dropout layer\n",
    "    def forward(self, inputs):\n",
    "        # Save the inputs in this layer\n",
    "        self.inputs = inputs\n",
    "        \n",
    "        # Create the mask using the bernoulli distribution with rate as the probability\n",
    "        # Dividing the self.rate keeps the total values of the input the same\n",
    "        self.binary_mask = np.random.binomial(1, self.rate, size=inputs.shape) / self.rate\n",
    "        \n",
    "        # Disable the randomly selected neurons\n",
    "        self.output = inputs * self.binary_mask\n",
    "        \n",
    "    # Back propagation for Drop out layer\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues * self.binary_mask\n",
    "\n",
    "# Create the Batch Normalization layer\n",
    "class Batch_norm:\n",
    "    \n",
    "    # Initiate the learnable parameters gamma and beta with default values\n",
    "    def __init__(self,gamma=0.99, beta=0.):\n",
    "        self.g = gamma\n",
    "        self.b = beta\n",
    "    \n",
    "    # Forward propagation\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        # Save the output from previous layer as inputs\n",
    "        self.inputs = inputs\n",
    "\n",
    "        # Get the mean, variance of this batch\n",
    "        self.x_mean = inputs.mean(axis=0)\n",
    "        self.x_var = inputs.var(axis=0)\n",
    "\n",
    "        # Normalize the batch using the equation from the paper\n",
    "        self.inv_var = 1/ (self.x_var + 1e-7)\n",
    "        output = (inputs - self.x_mean) * (self.inv_var) ** 0.5\n",
    "\n",
    "        # Save the output of this layer and to pass it to the next layer\n",
    "        self.output = output\n",
    "    \n",
    "    # Backward propagation\n",
    "    def backward(self, dvalues):\n",
    "        \n",
    "        # Get the number of samples and the dimension of the data\n",
    "        N, d = dvalues.shape\n",
    "\n",
    "        # Initiate the learnable parameters\n",
    "        self.gamma = np.full((N, d), self.g)\n",
    "        self.beta = np.full((N, d), self.b)\n",
    "        \n",
    "        # Assign the normalized values as x_til for simpler code\n",
    "        x_til = self.output\n",
    "\n",
    "        # intermediate partial derivatives\n",
    "        dx_til = dvalues * self.gamma\n",
    "\n",
    "        # final partial derivatives\n",
    "        dinputs = (1. / N) * self.inv_var  * (N*dx_til - np.sum(dx_til, axis=0)\n",
    "            - x_til*np.sum(dx_til*x_til, axis=0))\n",
    "        dbeta = np.sum(dvalues, axis=0)\n",
    "        dgamma = np.sum(x_til*dvalues, axis=0)\n",
    "        \n",
    "        # return the gradient and pass them to the next layer if suitable\n",
    "        self.dinputs = dinputs\n",
    "        self.d_gamma = dgamma\n",
    "        self.d_beta = dbeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Finished 0 epochs...\n",
      "acc:0.2316 |loss:2.3019 | lr: 0.0006\n",
      "Finished 5 epochs...\n",
      "acc:0.2669 |loss:2.3018 | lr: 0.0002\n",
      "Finished 10 epochs...\n",
      "acc:0.2771 |loss:2.3018 | lr: 0.0001\n",
      "Finished 15 epochs...\n",
      "acc:0.2822 |loss:2.3018 | lr: 0.0001\n",
      "Finished 20 epochs...\n",
      "acc:0.2841 |loss:2.3018 | lr: 0.0001\n",
      "Finished 25 epochs...\n",
      "acc:0.2870 |loss:2.3018 | lr: 0.0000\n",
      "Done training\n"
     ]
    }
   ],
   "source": [
    "#Set some of the paramaters for the training process\n",
    "epochs = 30\n",
    "batch_size = 128\n",
    "learning_rate = 0.001\n",
    "\n",
    "\n",
    "# Model\n",
    "# Add a Dense layer with input shape = 128 and conected to 64 hidden units, along with regularizer values\n",
    "dense1 = dense(128,64, w_regularizer_l2 = 5e-4,b_regularizer_l2 = 5e-4)\n",
    "# Add the Batch normalization layer\n",
    "batch_norm1 = Batch_norm()\n",
    "# Add a Relu activation layer\n",
    "activation1 = ReLu()\n",
    "# Add dropoutlayer with 0.1 rate\n",
    "dropout1 = Dropout(0.2)\n",
    "# Add a second Dense layer with 64 input_shape and output shape 10 to match the number of class \n",
    "dense2 = dense(64,16, w_regularizer_l2 = 5e-4,b_regularizer_l2 = 5e-4)\n",
    "\n",
    "\n",
    "batch_norm2 = Batch_norm()\n",
    "activation2 = ReLu()\n",
    "# Add a second Dense layer with 64 input_shape and output shape 10 to match the number of class \n",
    "dense3 = dense(16,10, w_regularizer_l2 = 5e-4,b_regularizer_l2 = 5e-4)\n",
    "activation3 = ReLu()\n",
    "\n",
    "# Add a softmax layer for the output to get the probability\n",
    "loss_activation = softmax_crossentropy_loss()\n",
    "\n",
    "# Set the stochastic gradient descent optimizer with learning rate, decaying rate and momentum\n",
    "optimizer = Optimizer_SGD(learning_rate, decay=0.002, momentum=0.9)\n",
    "\n",
    "# Apply Mini-batch\n",
    "def create_batches(inputs, targets, batchsize=64):\n",
    "\n",
    "    # Ensure the input and label has same length\n",
    "    assert inputs.shape[0] == targets.shape[0]\n",
    "\n",
    "    # Create a list of indices with length of the input samples\n",
    "    indices = np.arange(inputs.shape[0])\n",
    "\n",
    "    # Random sample the indices\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    # Yield the batches for training\n",
    "    for start_idx in range(0, inputs.shape[0] - batchsize + 1, batchsize):\n",
    "        excerpt = indices[start_idx:start_idx + batchsize]\n",
    "\n",
    "        yield inputs[excerpt], targets[excerpt]\n",
    "\n",
    "\n",
    "# Create list to save the accuracies and losses during training\n",
    "acc_train = []\n",
    "loss_train = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    # Mini-batch SGD\n",
    "    for batch in create_batches(X_train, y_train, batch_size):\n",
    "        x_batch, y_batch = batch\n",
    "        \n",
    "        # Forward propagation\n",
    "        # Use x_batch as the input\n",
    "        # Eacher layer takes the output of the previous layer\n",
    "        dense1.forward(x_batch)\n",
    "        batch_norm1.forward(dense1.output)\n",
    "        activation1.forward(batch_norm1.output)\n",
    "        dropout1.forward(activation1.output)\n",
    "        \n",
    "        dense2.forward(dropout1.output)\n",
    "        batch_norm2.forward(dense2.output)\n",
    "        activation2.forward(batch_norm2.output)\n",
    "\n",
    "        dense3.forward(activation2.output)\n",
    "        activation3.forward(dense3.output)\n",
    "\n",
    "        data_loss = loss_activation.forward(activation3.output, y_batch)\n",
    "\n",
    "        # Add up the regularization loss\n",
    "        regularization_loss = loss_activation.loss.regularization_loss(dense1) + loss_activation.loss.regularization_loss(dense2)\n",
    "\n",
    "        # Calculate the total loss of this epoch training for visualization later\n",
    "        loss = data_loss + regularization_loss\n",
    "\n",
    "        # Backpropagation\n",
    "        # From the output of the softmax layer\n",
    "        # Each layer will use gradient of previous layers and take derivatives of that with the input of this layer\n",
    "        # to get the gradient of this layer\n",
    "        loss_activation.backward(loss_activation.output, y_batch)\n",
    "\n",
    "        activation3.backward(loss_activation.dinputs)\n",
    "        dense3.backward(activation3.dinputs)\n",
    "\n",
    "        activation2.backward(dense3.dinputs)\n",
    "        batch_norm2.backward(activation2.dinputs)\n",
    "        dense2.backward(batch_norm2.dinputs)\n",
    "\n",
    "        dropout1.backward(dense2.dinputs)\n",
    "        activation1.backward(dropout1.dinputs)\n",
    "        batch_norm1.backward(activation1.dinputs)\n",
    "        dense1.backward(batch_norm1.dinputs)\n",
    "\n",
    "        # Update optimizer paramters\n",
    "        optimizer.pre_update()\n",
    "\n",
    "        # Update the weights and bias in the dense layers\n",
    "        optimizer.update_params(dense1)\n",
    "        optimizer.update_params(dense2)\n",
    "\n",
    "        # Update the learnable gamma and beta in batch normalization\n",
    "        optimizer.update_BN_param(batch_norm1)\n",
    "\n",
    "        # Iteration + 1 for SGD with momentum\n",
    "        optimizer.post_update()\n",
    "\n",
    "    # Make a forward pass using the training set and calculate the accuracy and loss\n",
    "    # dense1.forward(X_train)\n",
    "    # activation1.forward(dense1.output)\n",
    "    # dropout1.forward(dense1.output)\n",
    "    # dense2.forward(dropout1.output)\n",
    "    # val_loss = loss_activation.forward(dense2.output, y_train)\n",
    "\n",
    "    dense1.forward(X_train)\n",
    "    activation1.forward(dense1.output)\n",
    "    dropout1.forward(activation1.output)\n",
    "    \n",
    "    dense2.forward(dropout1.output)\n",
    "    activation2.forward(dense2.output)\n",
    "\n",
    "    dense3.forward(activation2.output)\n",
    "    activation3.forward(dense3.output)\n",
    "\n",
    "    val_loss = loss_activation.forward(activation3.output, y_train)\n",
    "\n",
    "    prediction = np.argmax(loss_activation.output, axis=1)\n",
    "    if len(y_train) == 2:\n",
    "        y_train = np.argmax(y_train, axis=1)\n",
    "    accuracy = np.mean(y_train==prediction)\n",
    "\n",
    "\n",
    "    # Print training accuracy, loss and learning rate every 5 epochs\n",
    "    if not epoch % 5 :\n",
    "        print(\"Finished {} epochs...\".format(epoch))\n",
    "        print(f\"acc:{accuracy:.4f} |loss:{val_loss:.4f} | lr: {optimizer.current_lr:.4f}\")\n",
    "    \n",
    "    # Save the accuracy and loss for visualization\n",
    "    acc_train.append(accuracy)\n",
    "    loss_train.append(val_loss)\n",
    "\n",
    "print(\"Done training\")\n",
    "\n",
    "\n",
    "# Evaluate on the test set\n",
    "# dense1.forward(X_test)\n",
    "# activation1.forward(dense1.output)\n",
    "# dense2.forward(activation1.output)\n",
    "# loss = loss_activation.forward(dense2.output, y_test)\n",
    "# prediction = np.argmax(loss_activation.output, axis=1)\n",
    "# if len(y_test) == 2:\n",
    "#     y_test = np.argmax(y_test, axis=1)\n",
    "# accuracy = np.mean(y_test==prediction)\n",
    "\n",
    "# print(\"Testing accuracy\")\n",
    "# print(\"-\"*50)\n",
    "# print(f\"acc:{accuracy:.4f} |loss:{loss:.4f}\")\n",
    "\n",
    "# # Plot the accuracy and loss per epoch to see the training progress\n",
    "# plt.plot(acc_train)\n",
    "# plt.title(\"Training accuracy vs epochs\")\n",
    "# plt.show()\n",
    "\n",
    "# plt.plot(loss_train)\n",
    "# plt.title(\"Loss vs epochs\")\n",
    "# plt.show()"
   ]
  },
  {
   "source": [
    "# Experiments\n",
    "\n",
    "- Learning rate = [0.05, 0.01, 0.005, 0.001]\n",
    "- Effect of weight decay\n",
    "- Effect of weight regularization\n",
    "- Effect of Dropout\n",
    "- Effect of batch normalization\n",
    "- Effect or one more hidden layer\n",
    "- Compare ReLu and Sigmoid\n",
    "- Momentum\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}