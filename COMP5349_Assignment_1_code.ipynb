{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "### Still need to implement and Batch_norm\n",
    "The tutorial does not work because it use a for loop to create the mode, forward pass and back propagation, assuming all have same operation, which is not the case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, random\n",
    "import numpy as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset and normalized\n"
     ]
    }
   ],
   "source": [
    "# Load in the training set\n",
    "X_train = np.load('Assignment1-Dataset/train_data.npy')\n",
    "y_train = np.load('Assignment1-Dataset/train_label.npy')\n",
    "# Reshape the label to a 1-d vector\n",
    "y_train = y_train.reshape(-1,)\n",
    "\n",
    "# # Load in the test set\n",
    "X_test = np.load('Assignment1-Dataset/test_data.npy')\n",
    "y_test = np.load('Assignment1-Dataset/test_label.npy')\n",
    "# Reshape the label to a 1-d vector\n",
    "y_test = y_test.reshape(-1,)\n",
    "\n",
    "# Normalize (optional)\n",
    "def normalize(X, X2):\n",
    "    X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "    X2 = (X2 - X.mean(axis=0)) / X.std(axis=0) # Normalize the data with the mean and sd from train set\n",
    "    return X, X2\n",
    "\n",
    "X_train, X_test = normalize(X_train, X_test)\n",
    "\n",
    "print(\"Loaded dataset and normalized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Dense Layer class\n",
    "class dense:\n",
    "    # Initiate the class with required input and output dimension, and regularization\n",
    "    def __init__(self, input_dim, output_dim, w_regularizer_l1=0, w_regularizer_l2=0, \n",
    "b_regularizer_l1=0, b_regularizer_l2=0):\n",
    "        \n",
    "        # Initiate the weights and bias\n",
    "        self.w = 0.01 * np.random.randn(input_dim, output_dim)\n",
    "        self.b = np.zeros((1, output_dim))\n",
    "        \n",
    "        # Initiate the regularizer lambda value\n",
    "        self.w_regularizer_l1 = w_regularizer_l1\n",
    "        self.w_regularizer_l2 = w_regularizer_l2\n",
    "        self.b_regularizer_l1 = b_regularizer_l1\n",
    "        self.b_regularizer_l2 = b_regularizer_l2\n",
    "    \n",
    "    # Forward propagation of the Dense layer\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs # Save the inputs when it is pased into the layer\n",
    "        self.output = inputs.dot(self.w) + self.b # Calculate the output of this later\n",
    "    \n",
    "    # Backward propagation of the Dense layer\n",
    "    def backward(self, dvalues):\n",
    "        # Get gradient on weights and bias\n",
    "        # Dot product of input transposed and the gradient from previous layer\n",
    "        self.d_w = np.dot(self.inputs.T, dvalues)\n",
    "        self.d_b = np.sum(dvalues, axis=0, keepdims=True) # Sum up the delta from previous layer\n",
    "        \n",
    "        # Add the regularization penalties calculated from the original weights if to the gradient of the weights\n",
    "        if self.w_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.w)\n",
    "            # For negative weights, the derivative of it is -1\n",
    "            dL1[self.w < 0] = -1\n",
    "            self.d_w += self.w_regularizer_l1 * dL1\n",
    "        \n",
    "        if self.w_regularizer_l2 > 0:\n",
    "            self.d_w += 2 * self.w_regularizer_l1 * self.w\n",
    "\n",
    "        # Add the regularization penalties calculated from the original bias if to the gradient of the weights\n",
    "        if self.b_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.b)\n",
    "            # For negative bias, the derivative of it is -1\n",
    "            dL1[self.b < 0] = -1\n",
    "            self.d_w += self.b_regularizer_l1 * dL1\n",
    "        \n",
    "        if self.b_regularizer_l2 > 0:\n",
    "            self.d_b += self.b_regularizer_l2 * self.b\n",
    "            \n",
    "        # Gradient wrt inputs to be backprogated to the next layer\n",
    "        self.dinputs = np.dot(dvalues, self.w.T)\n",
    "    \n",
    "# Create the ReLu activaton layer class\n",
    "class ReLu:\n",
    "    \n",
    "    # Forward propagation for ReLu\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs # Save the inputs passed into this later\n",
    "        self.output = np.maximum(0, inputs) # Apply the relu function on each inputs\n",
    "    \n",
    "    # Back propagation for Relu\n",
    "    def backward(self, dvalues):\n",
    "        # Use the copy of the gradient instead on changing them\n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[self.inputs <0] = 0 # Apply the derivative of ReLu to the gradient wrt to gradient from last layer\n",
    "\n",
    "# Create the Softmax layer class\n",
    "class softmax:\n",
    "    # Forward propagation for Softmax\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs # Save the inputs passed into this layer\n",
    "        \n",
    "        # Apply the softmax function to the input\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        prob = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        \n",
    "        # Save the output values (predicted probabilities of each class) in this layer\n",
    "        self.output = prob\n",
    "    \n",
    "    # Back propagation for Softmax\n",
    "    def backward(self, dvalues):\n",
    "        # Create an empty array with shape (n_class,)\n",
    "        self.inputs = np.empty_like(dvalues)\n",
    "        \n",
    "        # Calculating the gradient of Softmax layer using its derivative\n",
    "        for idx, (single_output, single_dvalue) in enumerate(zip(self.output, dvalues)):\n",
    "            single_output = single_output.reshape(-1, 1)\n",
    "            jacob_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "        self.dinputs[idx] = np.dot(jacob_matrix, single_dvalue)\n",
    "    \n",
    "# Create a general Loss class for different loss function\n",
    "class Loss:\n",
    "    \n",
    "    # Add l1, l2, loss to a lump sum regularization loss\n",
    "    def regularization_loss(self, layer):\n",
    "        regularization_loss = 0\n",
    "        # Add l1 loss of weights to regularization loss\n",
    "        if layer.w_regularizer_l1 > 0:\n",
    "            regularization_loss += layer.w_regularizer_l1 * np.sum(np.abs(layer.w))\n",
    "        # Add l2 loss of weights to regularization loss\n",
    "        if layer.w_regularizer_l2 > 0:\n",
    "            regularization_loss += layer.w_regularizer_l2 * np.sum(np.abs(layer.w * layer.w))\n",
    "        # Add l1 loss of biasa to regularization loss \n",
    "        if layer.b_regularizer_l1 > 0:\n",
    "            regularization_loss += layer.b_regularizer_l1 * np.sum(np.abs(layer.b))\n",
    "        # Add l2 loss of biasa to regularization loss\n",
    "        if layer.b_regularizer_l2 > 0:\n",
    "            regularization_loss += layer.b_regularizer_l1 * np.sum(np.abs(layer.b * layer.b))\n",
    "        \n",
    "        return regularization_loss\n",
    "    \n",
    "    # A vanilla way to calculate loss using the loss function without regularization\n",
    "    def calculate(self, output, y):\n",
    "        sample_losses = self.forward(output, y)\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        return data_loss\n",
    "    \n",
    "# Create Crossentropy loss class with inherit the generl Loss class\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "    \n",
    "    # Forward propagation of CrossEntropy loss function\n",
    "    # which takes the output of the softmax layer as y_pred\n",
    "    def forward(self, y_pred, y):\n",
    "        sample = len(y_pred)\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7) # clipped the prediction to avoid divisio by 0\n",
    "        \n",
    "        if len(y.shape) == 1:\n",
    "            # Produce the vector of the predicted probability of the correct label\n",
    "            correct_confidence = y_pred_clipped[range(sample), y] \n",
    "        if len(y.shape) == 2:\n",
    "            correct_confidence = np.sum(y_pred_clipped * y, axis=1)\n",
    "        \n",
    "        # Calculate the negative log likelihoods\n",
    "        negative_log_likelihoods = -np.log(correct_confidence)\n",
    "        return negative_log_likelihoods\n",
    "    \n",
    "    # Back propagation of CrossEntropy loss function\n",
    "    def backward(self, dvalues, y):\n",
    "        \n",
    "        # Get the number of predictions (length of y_pred)\n",
    "        sample = len(dvalues)\n",
    "        # Get the number of class in the prediction\n",
    "        labels = len(dvalues[0])\n",
    "        \n",
    "        # Create one-hot encoding if y is an index label\n",
    "        if len(y.shape) == 1:\n",
    "            y = np.eye(labels)[y]\n",
    "        \n",
    "        # Calculate the gradient wrt to input values using the derivative of the crossentropy loss\n",
    "        self.dinputs = -y / dvalues\n",
    "        \n",
    "        # Normalize the gradient according to the batch size\n",
    "        self.dinputs = self.dinputs / sample\n",
    "\n",
    "# Create the output layer as a combination of Softmax and CrossEntroly Loss\n",
    "class softmax_crossentropy_loss():\n",
    "    \n",
    "    # Initiate the layer with the chosen activation and loss function\n",
    "    def __init__(self):\n",
    "        self.activation = softmax()\n",
    "        self.loss = Loss_CategoricalCrossentropy()\n",
    "    \n",
    "    # Forward propogation for this layer\n",
    "    def forward(self, inputs, y, test=False):\n",
    "            \n",
    "            # Get predictsion by pass forwad in the Softmax class\n",
    "            # with the input it received from the previous layer\n",
    "            self.activation.forward(inputs)\n",
    "            \n",
    "            # Save the output values (prediction) in this layer\n",
    "            self.output = self.activation.output\n",
    "            \n",
    "            # If we are doing testing, no more follow up action\n",
    "            if test == True:\n",
    "                pass\n",
    "            # During training, use the chosen loss function to get the loss\n",
    "            elif test == False:\n",
    "                return self.loss.calculate(self.output, y)\n",
    "    \n",
    "    # Back propagation for this layer\n",
    "    def backward(self, dvalues, y):\n",
    "        sample = len(dvalues)\n",
    "        \n",
    "        # In case the predicted values are vector\n",
    "        # Get the index with highest probability\n",
    "        if len(y) == 2:\n",
    "            y = np.argmax(y, axis=1)\n",
    "        \n",
    "        # Calculate the gradient of from the cross entropy loss\n",
    "        # dinputs and dvalues here are the output (prediction) from the output layer (softmax)\n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[range(sample), y] -= 1 # Calculate the gradient of loss and softmax using chain rule\n",
    "        self.dinputs = self.dinputs / sample # Normalize the gradient\n",
    "\n",
    "# Create the Optimizer (Stochastic gradient descent) class\n",
    "class Optimizer_SGD:\n",
    "    \n",
    "    # Initiate the class with the learning rate, decay rate and momentum\n",
    "    def __init__(self, lr=0.1, decay=0., momentum=0.):\n",
    "        self.lr = lr\n",
    "        self.current_lr = lr\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.momentum = momentum\n",
    "    \n",
    "    # Precall the decay function\n",
    "    # Since interation initiated at 0, the learning rate is still the preset one\n",
    "    def pre_update(self):\n",
    "        # Continue update the learning rate with the decay rate after each epochs\n",
    "        if self.decay:\n",
    "            self.current_lr = self.lr * (1 / (1 + self.decay * self.iterations))\n",
    "    \n",
    "    # Update the weights and bias with the gradient from back propagation\n",
    "    def update_params(self, layer):\n",
    "        \n",
    "        # If there is a momentum, we apply momentun to the updates\n",
    "        if self.momentum:\n",
    "            if not hasattr(layer, 'weight_momentums'):\n",
    "                \n",
    "                # Momentum at start is 0\n",
    "                layer.weight_momentums = np.zeros_like(layer.w)\n",
    "                layer.bias_momentum = np.zeros_like(layer.b)\n",
    "            \n",
    "            # The negative update is increased by the momentum\n",
    "            w_updates = self.momentum * layer.weight_momentums - self.current_lr * layer.d_w\n",
    "            \n",
    "            # Update the momentum from the update after each epoch\n",
    "            layer.weight_momentums = w_updates\n",
    "            \n",
    "            # Similar handling for bias\n",
    "            b_updates = self.momentum * layer.bias_momentum - self.current_lr * layer.d_b\n",
    "            layer.bias_momentum = b_updates\n",
    "        \n",
    "        else:\n",
    "            # Regular updates by multiplying the gradient with the learning rate\n",
    "            w_updates -= self.current_lr * layer.d_w\n",
    "            b_updates -= self.current_lr * layer.d_b\n",
    "        \n",
    "        # Update the weights and bias\n",
    "        layer.w += w_updates\n",
    "        layer.b += b_updates\n",
    "        \n",
    "    def update_BN_param(self, layer):\n",
    "        \n",
    "        # If there is a momentum, we apply momentun to the updates\n",
    "        if self.momentum:\n",
    "            if not hasattr(layer, 'gamma_momentums'):\n",
    "                \n",
    "                # Momentum at start is 0\n",
    "                layer.gamma_momentums = np.zeros_like(layer.gamma)\n",
    "                layer.beta_momentum = np.zeros_like(layer.beta)\n",
    "            \n",
    "            # The negative update is increased by the momentum\n",
    "            gamma_updates = self.momentum * layer.gamma_momentums - self.current_lr * layer.d_gamma\n",
    "            \n",
    "            # Update the momentum from the update after each epoch\n",
    "            layer.gamma_momentums = gamma_updates\n",
    "            \n",
    "            # Similar handling for bias\n",
    "            beta_updates = self.momentum * layer.beta_momentum - self.current_lr * layer.d_beta\n",
    "            layer.beta_momentum = beta_updates\n",
    "        \n",
    "        else:\n",
    "            # Regular updates by multiplying the gradient with the learning rate\n",
    "            gamma_updates -= self.current_lr * layer.d_gamma\n",
    "            beta_updates -= self.current_lr * layer.d_beta\n",
    "        \n",
    "        # Update the weights and bias\n",
    "        layer.gamma += gamma_updates\n",
    "        layer.beta += beta_updates\n",
    "    \n",
    "    # Keep track of the number of epoches\n",
    "    def post_update(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "# Create the Dropout layer\n",
    "class Dropout:\n",
    "    # Initiate layer with drop out rate (drop out here is the ratio of units to be disabled)\n",
    "    def __init__(self, rate):\n",
    "        self.rate = 1-rate\n",
    "    \n",
    "    # Forward propagation for dropout layer\n",
    "    def forward(self, inputs):\n",
    "        # Save the inputs in this layer\n",
    "        self.inputs = inputs\n",
    "        \n",
    "        # Create the mask using the bernoulli distribution with rate as the probability\n",
    "        # Dividing the self.rate keeps the total values of the input the same\n",
    "        self.binary_mask = np.random.binomial(1, self.rate, size=inputs.shape) / self.rate\n",
    "        \n",
    "        # Disable the randomly selected neurons\n",
    "        self.output = inputs * self.binary_mask\n",
    "        \n",
    "    # Back propagation for Drop out layer\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues * self.binary_mask\n",
    "\n",
    "# Create the Batch Normalization layer\n",
    "class Batch_norm:\n",
    "    \n",
    "    def __init__(self,gamma=0.99, beta=0.):\n",
    "        self.g = gamma\n",
    "        self.b = beta\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.x_mean = inputs.mean(axis=0)\n",
    "        self.x_var = inputs.var(axis=0)\n",
    "        self.inv_var = 1/ (self.x_var + 1e-7)\n",
    "        output = (inputs - self.x_mean) * (self.inv_var) ** 0.5\n",
    "        self.output = output\n",
    "    \n",
    "    def backward(self, dvalues):\n",
    "\n",
    "        N, d = dvalues.shape\n",
    "        self.gamma = np.full((N, d), self.g)\n",
    "        self.beta = np.full((N, d), self.b)\n",
    "        \n",
    "        x_til = self.output\n",
    "\n",
    "        # intermediate partial derivatives\n",
    "        dx_til = dvalues * self.gamma\n",
    "\n",
    "        # final partial derivatives\n",
    "        dinputs = (1. / N) * self.inv_var  * (N*dx_til - np.sum(dx_til, axis=0)\n",
    "            - x_til*np.sum(dx_til*x_til, axis=0))\n",
    "        dbeta = np.sum(dvalues, axis=0)\n",
    "        dgamma = np.sum(x_til*dvalues, axis=0)\n",
    "        \n",
    "        self.dinputs = dinputs\n",
    "        self.d_gamma = dgamma\n",
    "        self.d_beta = dbeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "dense1 = dense(128,32, w_regularizer_l2 = 5e-4,b_regularizer_l2 = 5e-4)\n",
    "# Add the Batch normalization layer\n",
    "batch_norm1 = Batch_norm()\n",
    "activation1 = ReLu()\n",
    "dropout1 = Dropout(0)\n",
    "dense2 = dense(32,20)\n",
    "loss_activation = softmax_crossentropy_loss()\n",
    "\n",
    "optimizer = Optimizer_SGD(lr=0.005, decay=0.002, momentum=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 0 epochs...\n",
      "acc:0.3265 |loss:2.2829 | lr: 0.0020\n",
      "Finished 10 epochs...\n",
      "acc:0.3996 |loss:1.6511 | lr: 0.0003\n",
      "Finished 20 epochs...\n",
      "acc:0.4039 |loss:1.6671 | lr: 0.0001\n",
      "Finished 30 epochs...\n",
      "acc:0.4030 |loss:1.5325 | lr: 0.0001\n",
      "Finished 40 epochs...\n",
      "acc:0.4019 |loss:1.5487 | lr: 0.0001\n",
      "Done training\n"
     ]
    }
   ],
   "source": [
    "def create_batches(inputs, targets, batchsize):\n",
    "    assert inputs.shape[0] == targets.shape[0]\n",
    "    indices = np.arange(inputs.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    for start_idx in range(0, inputs.shape[0] - batchsize + 1, batchsize):\n",
    "        excerpt = indices[start_idx:start_idx + batchsize]\n",
    "\n",
    "        yield inputs[excerpt], targets[excerpt]\n",
    "        \n",
    "for epoch in range(50):\n",
    "    # Mini-batch SGD\n",
    "    batch_size = 64\n",
    "#     n_batch = math.floor(len(X_train) / batch_size)\n",
    "#     for i in range(n_batch):\n",
    "    for batch in create_batches(X_train, y_train, batch_size):\n",
    "        x_batch, y_batch = batch\n",
    "#         x_batch = X_train[i*n_batch:(i+1)*n_batch]\n",
    "#         y_batch = y_train[i*n_batch:(i+1)*n_batch]\n",
    "        \n",
    "        dense1.forward(x_batch)\n",
    "        # Add BN\n",
    "#         batch_norm1.forward(dense1.output)\n",
    "        activation1.forward(dense1.output)\n",
    "        dropout1.forward(activation1.output)\n",
    "        dense2.forward(dropout1.output)\n",
    "        data_loss = loss_activation.forward(dense2.output, y_batch)\n",
    "\n",
    "        regularization_loss = loss_activation.loss.regularization_loss(dense1) + loss_activation.loss.regularization_loss(dense2)\n",
    "        loss = data_loss + regularization_loss\n",
    "\n",
    "        # Backpropagation\n",
    "        loss_activation.backward(loss_activation.output, y_batch)\n",
    "        dense2.backward(loss_activation.dinputs)\n",
    "        dropout1.backward(dense2.dinputs)\n",
    "        activation1.backward(dropout1.dinputs)\n",
    "#         batch_norm1.backward(activation1.dinputs)\n",
    "        dense1.backward(activation1.dinputs)\n",
    "\n",
    "        optimizer.pre_update()\n",
    "        optimizer.update_params(dense1)\n",
    "        optimizer.update_params(dense2)\n",
    "#         optimizer.update_BN_param(batch_norm1)\n",
    "        optimizer.post_update()\n",
    "\n",
    "    dense1.forward(X_train)\n",
    "    activation1.forward(dense1.output)\n",
    "    dropout1.forward(dense1.output)\n",
    "    dense2.forward(dropout1.output)\n",
    "    data_loss = loss_activation.forward(dense2.output, y_train)\n",
    "    prediction = np.argmax(loss_activation.output, axis=1)\n",
    "    if len(y_train) == 2:\n",
    "        y_train = np.argmax(y_train, axis=1)\n",
    "    accuracy = np.mean(y_train==prediction)\n",
    "\n",
    "\n",
    "#     print ( f'epoch: {epoch},' + f'acc: {accuracy:.3f},' + f'loss: {loss:.3f }' )\n",
    "    if not epoch % 10 :\n",
    "        print(\"Finished {} epochs...\".format(epoch))\n",
    "        print(f\"acc:{accuracy:.4f} |loss:{loss:.4f} | lr: {optimizer.current_lr:.4f}\")\n",
    "\n",
    "print(\"Done training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy\n",
      "--------------------------------------------------\n",
      "acc:0.3463 |loss:2.8832\n"
     ]
    }
   ],
   "source": [
    "dense1.forward(X_test)\n",
    "activation1.forward(dense1.output)\n",
    "dense2.forward(activation1.output)\n",
    "loss = loss_activation.forward(dense2.output, y_test)\n",
    "prediction = np.argmax(loss_activation.output, axis=1)\n",
    "if len(y_test) == 2:\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "accuracy = np.mean(y_test==prediction)\n",
    "\n",
    "print(\"Testing accuracy\")\n",
    "print(\"-\"*50)\n",
    "print(f\"acc:{accuracy:.4f} |loss:{loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
