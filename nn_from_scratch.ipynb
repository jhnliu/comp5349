{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "## Still need to implement momentum and mini-batch SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 128)\n",
      "(10000, 128)\n"
     ]
    }
   ],
   "source": [
    "# Load in the training set\n",
    "X_train = np.load('Assignment1-Dataset/train_data.npy')\n",
    "y_train = np.load('Assignment1-Dataset/train_label.npy')\n",
    "\n",
    "# # Load in the test set\n",
    "X_test = np.load('Assignment1-Dataset/test_data.npy')\n",
    "y_test = np.load('Assignment1-Dataset/test_label.npy')\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "# Normalize (optional)\n",
    "def normalize(X):\n",
    "    X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "    return X\n",
    "X_train = normalize(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN:\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, n_hidden_layer, batch_size):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.n_hidden_layer = n_hidden_layer\n",
    "        self.network = self._build_network()\n",
    "\n",
    "    # < ---- Basic numpy functions ---- > #\n",
    "    # Sigmoid activation function\n",
    "    def _sigmoid(self, x):\n",
    "        return 1.0/(1.0+math.exp(-x))\n",
    "\n",
    "    # Sigmoid derivative\n",
    "    def _sigmoid_derivative(self, sigmoid):\n",
    "        return sigmoid*(1.0-sigmoid)\n",
    "\n",
    "    # One-hot encoding\n",
    "    def _one_hot_encoding(self, idx, output_dim):\n",
    "        x = np.zeros(output_dim, dtype=np.int)\n",
    "        x[idx] = 1\n",
    "        return x\n",
    "\n",
    "    # ReLu activation function\n",
    "    def _relu(self, x):\n",
    "        return max(0, x)\n",
    "\n",
    "    # ReLu derivative\n",
    "    def _relu_derivative(self, x):\n",
    "        if x < 0: return 0\n",
    "        else: return 1\n",
    "    \n",
    "    # < ---- Construct the Batch Normalization Layer ---- >\n",
    "    def batch_normalize(self, x):\n",
    "        norm = np.linalg.norm(x)\n",
    "        if norm == 0: return x\n",
    "        else: return x / norm\n",
    "        \n",
    "    # Define the drop out layer\n",
    "    def drop_out(self, x, rate):\n",
    "        # Create a random list of index that the node input will be set to 0\n",
    "        set_0 = round(len(x) * rate)\n",
    "        \n",
    "        #Set the input to 0\n",
    "        for i in random.sample(range(len(x)), set_0):\n",
    "            x[i] = 0\n",
    "        return x\n",
    "\n",
    "    # < ---- Building the network architecture ---- > #\n",
    "    def _build_network(self):\n",
    "        \n",
    "        # Create a single fuly connected layer\n",
    "        def fc_layers(input_dim, output_dim):\n",
    "            layer = []\n",
    "            for i in range(output_dim): # Add a weight between each node and unit\n",
    "                weights= [random.random() for _ in range(input_dim)] # Determine FC layer with randomised/normalized w initialization\n",
    "                node = {\"weight\" : weights, \n",
    "                         \"output\": None,\n",
    "                         \"delta\": None}\n",
    "                layer.append(node) # Create the layer\n",
    "            return layer\n",
    "\n",
    "        network = [] # Build the network layer by layer\n",
    "        \n",
    "        # If there is no hidden layer, we only have one output layer\n",
    "        if len(self.n_hidden_layer) == 0:\n",
    "            network.append(fc_layers(self.input_dim, self.output_dim))\n",
    "            \n",
    "        # Otherwise, we will add hidden layers\n",
    "        else:\n",
    "            # First add a fully connected layer with n hidden units in the layer\n",
    "            # input dim is the input from the previous layer\n",
    "            # n_hidden_layer is the output of this layer aka the hidden units\n",
    "            network.append(fc_layers(self.input_dim, self.n_hidden_layer[0]))\n",
    "            \n",
    "            # Add n layers with each layer output a n-1 dimension\n",
    "#             for i in range(1, self.n_hidden_layer[0]):\n",
    "#             for i in range(1,3):\n",
    "            network.append(fc_layers(self.n_hidden_layer[0], self.n_hidden_layer[0]))\n",
    "                \n",
    "            # Add the output layer with output_dim == n_classes\n",
    "            network.append(fc_layers(self.n_hidden_layer[0], self.output_dim))\n",
    "\n",
    "        return network\n",
    "\n",
    "    # < ---- Training the model ---- > #\n",
    "\n",
    "    # Training the network\n",
    "    def train(self, X, y, n_epochs=100, lr=0.005, batch_size=256):\n",
    "        for epoch in range(n_epochs):\n",
    "            print(\"Training {} epoch\".format(epoch+1))\n",
    "            # Mini-batch training\n",
    "            n_batches = round(len(y)/batch_size)\n",
    "            for i in range(n_batches):\n",
    "                for _X, _y, in zip(X[i*batch_size:(i+1)*batch_size], y[i*batch_size:(i+1)*batch_size]):\n",
    "                    y_label = self._one_hot_encoding(_y, self.output_dim)\n",
    "                    self._forward_pass(_X)\n",
    "                    self._back_propagation(y_label)\n",
    "                    self._update_weights(_X, lr)\n",
    "                    \n",
    "\n",
    "    # Forward-pass function\n",
    "    def _forward_pass(self, x):\n",
    "        # Set function for relu and sigmoid activation\n",
    "        relu = self._relu\n",
    "        transfer = self._sigmoid\n",
    "        # Set input\n",
    "        x_in = x\n",
    "        \n",
    "        # Starting from the first layer, for each layer we forward pass the input\n",
    "        for layer in self.network[:-1]:\n",
    "            \n",
    "            # Create a list to save the output vector\n",
    "            x_out = []\n",
    "            \n",
    "            # For each node in the layer, we collect the dot product of the weights and the product\n",
    "            for node in layer:\n",
    "                # Get inner product of the input and weights and use the ReLu activation\n",
    "                node['output'] = relu(np.dot(node['weight'], x_in))\n",
    "                x_out.append(node['output'])\n",
    "            \n",
    "            # Add a batch normalization layer\n",
    "            x_in = self.batch_normalize(x_out) # Pass the output of this layer as the input to the next layer\n",
    "        \n",
    "        # Similar handling for the last output later\n",
    "        x_out = []\n",
    "        for node in self.network[-1]:\n",
    "            # Get inner product of the input and weights and use the Sigmoid activation\n",
    "            node['output'] = transfer(np.dot(node['weight'], x_in))\n",
    "            x_out.append(node['output'])\n",
    "        x_in = x_out # Pass the output of this layer as the input to the next layer\\\n",
    "\n",
    "        return x_in\n",
    "\n",
    "\n",
    "    # Back propagation function\n",
    "    def _back_propagation(self, y_label):\n",
    "        relu_derivative = self._relu_derivative\n",
    "        transfer_derivative = self._sigmoid_derivative\n",
    "        n_layers = len(self.network) # 7\n",
    "        for i in reversed(range(n_layers)): # i = range(7)\n",
    "            # Backpropagate from the output later\n",
    "            if i == n_layers - 1: # if i == 6 (the last layer)\n",
    "                for j, node in enumerate(self.network[i]):\n",
    "                    err = node['output'] - y_label[j]\n",
    "                    node['delta'] = err * transfer_derivative(node['output'])\n",
    "            else:\n",
    "                # Weighted sum of gradient from upper layer\n",
    "                for j, node in enumerate(self.network[i]):\n",
    "                    err = sum([node_['weight'][j] * node_['delta'] for node_ in self.network[i+1]])\n",
    "                    node['delta'] = err * relu_derivative(node['output'])\n",
    "\n",
    "    def _update_weights(self, x, lr):\n",
    "        for i, layer in enumerate(self.network):\n",
    "            if i == 0:\n",
    "                inputs = x\n",
    "            else:\n",
    "                inputs = [node_['output'] for node_ in self.network[i-1]]\n",
    "\n",
    "                # Update weights\n",
    "                for node in layer:\n",
    "                    for j, inpt in enumerate(inputs):\n",
    "                        node['weight'][j] -= node['delta'] * lr * inpt\n",
    "\n",
    "    # < ---- Making predictions ---- #\n",
    "    def predict(self, x):\n",
    "        pred = np.array([np.argmax(self._forward_pass(_x)) for _x in x], dtype=np.int)\n",
    "        return pred\n",
    "\n",
    "    # < ---- Define a new optimizer here ---- > #\n",
    "    def optimizer():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Data description --->  X.shape = (50000, 128), y.shape = (50000, 1), n_classes = 10\n",
      "\n",
      "Model details:\n",
      " input_dim = 128\n",
      " hidden_layers = [20]\n",
      " output_dim = 10\n",
      " batch_size = 128\n",
      " learning rate = 0.01\n",
      " n_epochs = 1\n",
      "Number of layers: 3\n"
     ]
    }
   ],
   "source": [
    "def accuracy(x, y):\n",
    "    count = 0\n",
    "    assert len(x) == len(y)\n",
    "    for i in range(len(x)):\n",
    "        if x[i] == y[i]: count += 1\n",
    "    acc = count / len(x)\n",
    "    return acc\n",
    "# Set up\n",
    "hidden_layers = [20] # number of nodes in hidden layers i.e. [layer1, layer2, ...]\n",
    "lr = 0.01 # learning rate\n",
    "n_epochs = 1 # number of training epoch\n",
    "batch_size = 128\n",
    "N, d = X_train.shape\n",
    "n_classes = len(np.unique(y_train))\n",
    "\n",
    "print(\" Data description --->  X.shape = {}, y.shape = {}, n_classes = {}\\n\".format(X_train.shape, y_train.shape, n_classes))\n",
    "print(\"Model details:\")\n",
    "print(\" input_dim = {}\".format(d))\n",
    "print(\" hidden_layers = {}\".format(hidden_layers))\n",
    "print(\" output_dim = {}\".format(n_classes))\n",
    "print(\" batch_size = {}\".format(batch_size))\n",
    "print(\" learning rate = {}\".format(lr))\n",
    "print(\" n_epochs = {}\".format(n_epochs))\n",
    "\n",
    "\n",
    "# Build neural network classifier model and train\n",
    "model = NN(input_dim=d, output_dim=n_classes, n_hidden_layer=hidden_layers, batch_size=batch_size) #, seed=seed_weights\n",
    "\n",
    "print(\"Number of layers:\", len(model.network))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Training model......\n",
      "Training 1 epoch\n",
      "\n",
      "------------------------------------------------------\n",
      "Calculating accuracies.....\n",
      "0.1\n",
      "0.1\n"
     ]
    }
   ],
   "source": [
    "print(\"------------------------------------------------------\")\n",
    "print(\"Training model......\")\n",
    "model.train(X_train, y_train, lr=lr, n_epochs=n_epochs)\n",
    "\n",
    "# Make predictions for training and test data\n",
    "ypred_train = model.predict(X_train)\n",
    "ypred_test = model.predict(X_test)\n",
    "print()\n",
    "print(\"------------------------------------------------------\")\n",
    "print(\"Calculating accuracies.....\")\n",
    "acc_train = accuracy(y_train, ypred_train)\n",
    "acc_test = accuracy(y_test, ypred_test)\n",
    "print(acc_train)\n",
    "print(acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
