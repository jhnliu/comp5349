{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "## Still need to implement momentum and mini-batch SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, random\n",
    "import numpy as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 128)\n",
      "(10000, 128)\n"
     ]
    }
   ],
   "source": [
    "# Load in the training set\n",
    "X_train = np.load('Assignment1-Dataset/train_data.npy')\n",
    "y_train = np.load('Assignment1-Dataset/train_label.npy')\n",
    "\n",
    "# # Load in the test set\n",
    "X_test = np.load('Assignment1-Dataset/test_data.npy')\n",
    "y_test = np.load('Assignment1-Dataset/test_label.npy')\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "# Normalize (optional)\n",
    "def normalize(X, X2):\n",
    "    X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "    X2 = (X2 - X.mean(axis=0)) / X.std(axis=0)\n",
    "    return X, X2\n",
    "X_train, X_test = normalize(X_train, X_test)\n",
    "# X_test = normalize(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN:\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, n_hidden_layer, batch_size):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.n_hidden_layer = n_hidden_layer\n",
    "        self.network = self._build_network()\n",
    "\n",
    "    # < ---- Basic numpy functions ---- > #\n",
    "    # Sigmoid activation function\n",
    "    def _sigmoid(self, x):\n",
    "        return 1.0/(1.0+math.exp(-x))\n",
    "\n",
    "    # Sigmoid derivative\n",
    "    def _sigmoid_derivative(self, sigmoid):\n",
    "        return sigmoid*(1.0-sigmoid)\n",
    "\n",
    "    # One-hot encoding\n",
    "    def _one_hot_encoding(self, idx, output_dim):\n",
    "        x = np.zeros(output_dim, dtype=np.int)\n",
    "        x[idx] = 1\n",
    "        return x\n",
    "\n",
    "    # ReLu activation function\n",
    "    def _relu(self, x):\n",
    "        return max(0, x)\n",
    "\n",
    "    # ReLu derivative\n",
    "    def _relu_derivative(self, x):\n",
    "        if x < 0: return 0\n",
    "        else: return 1\n",
    "    \n",
    "    # < ---- Construct the Batch Normalization Layer ---- >\n",
    "    def batch_normalize(self, x):\n",
    "        norm = np.linalg.norm(x)\n",
    "        if norm == 0: return x\n",
    "        else: return x / norm\n",
    "        \n",
    "    # Define the drop out layer\n",
    "    def drop_out(self, x, rate):\n",
    "        # Create a random list of index that the node input will be set to 0\n",
    "        set_0 = round(len(x) * rate)\n",
    "        \n",
    "        #Set the input to 0\n",
    "        for i in random.sample(range(len(x)), set_0):\n",
    "            x[i] = 0\n",
    "        return x\n",
    "            \n",
    "    def accuracy(self, x, y):\n",
    "        count = 0\n",
    "        assert len(x) == len(y)\n",
    "        for i in range(len(x)):\n",
    "            if x[i] == y[i]: count += 1\n",
    "        acc = count / len(x)\n",
    "        return acc\n",
    "\n",
    "    # < ---- Building the network architecture ---- > #\n",
    "    def _build_network(self):\n",
    "        \n",
    "        # Create a single fuly connected layer\n",
    "        def fc_layers(input_dim, output_dim):\n",
    "            layer = []\n",
    "            for i in range(output_dim): # Add a weight between each node and unit\n",
    "                weights= [random.random() for _ in range(input_dim)] # Determine FC layer with randomised/normalized w initialization\n",
    "                node = {\"weight\" : weights, \n",
    "                         \"output\": None,\n",
    "                         \"delta\": None}\n",
    "                layer.append(node) # Create the layer\n",
    "            return layer\n",
    "\n",
    "        network = [] # Build the network layer by layer\n",
    "        # If there is no hidden layer, we only have one output layer\n",
    "        if len(self.n_hidden_layer) == 0:\n",
    "            network.append(fc_layers(self.input_dim, self.output_dim))\n",
    "            \n",
    "        # Otherwise, we will add hidden layers\n",
    "        else:\n",
    "            # First add a fully connected layer with n hidden units in the layer\n",
    "            # input dim is the input from the previous layer\n",
    "            # n_hidden_layer is the output of this layer aka the hidden units\n",
    "            network.append(fc_layers(self.input_dim, self.n_hidden_layer[0]))\n",
    "            \n",
    "            # Add layers with each layer output a n-1 dimension\n",
    "#             network.append(fc_layers(self.n_hidden_layer[0], self.n_hidden_layer[0]))\n",
    "#             network.append(fc_layers(64, 32))\n",
    "            network.append(fc_layers(self.n_hidden_layer[0], self.n_hidden_layer[0]))\n",
    "            # Add the output layer with output_dim == n_classes\n",
    "            network.append(fc_layers(self.n_hidden_layer[0], self.output_dim))\n",
    "\n",
    "        return network\n",
    "\n",
    "    # < ---- Training the model ---- > #\n",
    "\n",
    "    # Training the network\n",
    "    def train(self, X, y, n_epochs=100, lr=0.005, batch_size=256):\n",
    "        print(\"------------------------------------------------------\")\n",
    "        print(\"Training model......\")\n",
    "        acc = []\n",
    "        for epoch in range(n_epochs):\n",
    "            print(\"------------------------------------------------------\")\n",
    "            start = datetime.now()\n",
    "            print(\"Epoch {}:\".format(epoch+1))\n",
    "            # Mini-batch training\n",
    "            n_batches = round(len(y)/batch_size)\n",
    "            for i in range(n_batches):\n",
    "                for _X, _y, in zip(X[i*batch_size:(i+1)*batch_size], y[i*batch_size:(i+1)*batch_size]):\n",
    "                    y_label = self._one_hot_encoding(_y, self.output_dim)\n",
    "                    self._forward_pass(_X)\n",
    "                    self._back_propagation(y_label)\n",
    "                    self._update_weights(_X, lr)\n",
    "                    \n",
    "            end = datetime.now()\n",
    "            # Make predictions for training and test data\n",
    "            ypred_train = self.predict(X)\n",
    "            acc_train = self.accuracy(y, ypred_train)\n",
    "            \n",
    "            ypred_test = self.predict(X_test)\n",
    "            acc_test = self.accuracy(y_test, ypred_test)\n",
    "            \n",
    "            print(\"Train: {}\".format(acc_train), \"Test: {}\".format(acc_test), \"Duration: {}\".format(end-start))\n",
    "            acc.append(acc_train)\n",
    "            \n",
    "        return acc\n",
    "\n",
    "    # Forward-pass function\n",
    "    def _forward_pass(self, x):\n",
    "        # Set function for relu and sigmoid activation\n",
    "        relu = self._relu\n",
    "        transfer = self._sigmoid\n",
    "        # Set input\n",
    "        x_in = x\n",
    "        \n",
    "        # Starting from the first layer, for each layer we forward pass the input\n",
    "        for layer in self.network[:-1]:\n",
    "            # Create a list to save the output vector\n",
    "            x_out = []\n",
    "            # For each node in the layer, we collect the dot product of the weights and the product\n",
    "            for node in layer:\n",
    "                # Get inner product of the input and weights and use the ReLu activation\n",
    "                node['output'] = transfer(np.dot(node['weight'], x_in))\n",
    "                x_out.append(node['output'])\n",
    "#             x_in = self.drop_out(x_out, 0.2)\n",
    "            # Add a batch normalization layer\n",
    "#             x_in = self.batch_normalize(x_out) # Pass the output of this layer as the input to the next layer\n",
    "            x_in = x_out\n",
    "        # Similar handling for the last output later\n",
    "        x_out = []\n",
    "        for node in self.network[-1]:\n",
    "            # Get inner product of the input and weights and use the Sigmoid activation\n",
    "            node['output'] = transfer(np.dot(node['weight'], x_in))\n",
    "            x_out.append(node['output'])\n",
    "        x_in = x_out # Pass the output of this layer as the input to the next layer\\\n",
    "\n",
    "        return x_in\n",
    "\n",
    "    # Back propagation function\n",
    "    def _back_propagation(self, y_label):\n",
    "        relu_derivative = self._relu_derivative #for relu activations\n",
    "        transfer_derivative = self._sigmoid_derivative # for sigmoid activations\n",
    "        n_layers = len(self.network) # 7\n",
    "        for i in reversed(range(n_layers)): # i = range(7)\n",
    "            # Backpropagate from the output later\n",
    "            if i == n_layers - 1: # if i == 6 (the last layer)\n",
    "                for j, node in enumerate(self.network[i]):\n",
    "                    err = node['output'] - y_label[j]\n",
    "                    node['delta'] = err * transfer_derivative(node['output'])\n",
    "            else:\n",
    "                # Weighted sum of gradient from upper layer\n",
    "                for j, node in enumerate(self.network[i]):\n",
    "                    err = sum([node_['weight'][j] * node_['delta'] for node_ in self.network[i+1]])\n",
    "                    node['delta'] = err * transfer_derivative(node['output'])\n",
    "\n",
    "    def _update_weights(self, x, lr):\n",
    "        for i, layer in enumerate(self.network):\n",
    "            if i == 0:\n",
    "                inputs = x\n",
    "            else:\n",
    "                inputs = [node_['output'] for node_ in self.network[i-1]]\n",
    "\n",
    "                # Update weights\n",
    "                for node in layer:\n",
    "                    for j, inpt in enumerate(inputs):\n",
    "                        node['weight'][j] -= node['delta'] * lr * inpt\n",
    "\n",
    "    # < ---- Making predictions ---- #\n",
    "    def predict(self, x):\n",
    "        pred = np.array([np.argmax(self._forward_pass(_x)) for _x in x], dtype=np.int)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Data description --->  X.shape = (50000, 128), y.shape = (50000, 1), n_classes = 10\n",
      "\n",
      "Model details:\n",
      " input_dim = 128\n",
      " hidden_layers = [16]\n",
      " output_dim = 10\n",
      " batch_size = 128\n",
      " learning rate = 0.05\n",
      " n_epochs = 50\n",
      "Number of layers: 3\n"
     ]
    }
   ],
   "source": [
    "def accuracy(x, y):\n",
    "    count = 0\n",
    "    assert len(x) == len(y)\n",
    "    for i in range(len(x)):\n",
    "        if x[i] == y[i]: count += 1\n",
    "    acc = count / len(x)\n",
    "    return acc\n",
    "# Set up\n",
    "hidden_layers = [16] # number of nodes in hidden layers i.e. [layer1, layer2, ...]\n",
    "lr = 0.05 # learning rate\n",
    "n_epochs = 50 # number of training epoch\n",
    "batch_size = 128\n",
    "N, d = X_train.shape\n",
    "n_classes = len(np.unique(y_train))\n",
    "\n",
    "print(\" Data description --->  X.shape = {}, y.shape = {}, n_classes = {}\\n\".format(X_train.shape, y_train.shape, n_classes))\n",
    "print(\"Model details:\")\n",
    "print(\" input_dim = {}\".format(d))\n",
    "print(\" hidden_layers = {}\".format(hidden_layers))\n",
    "print(\" output_dim = {}\".format(n_classes))\n",
    "print(\" batch_size = {}\".format(batch_size))\n",
    "print(\" learning rate = {}\".format(lr))\n",
    "print(\" n_epochs = {}\".format(n_epochs))\n",
    "\n",
    "\n",
    "# Build neural network classifier model and train\n",
    "model = NN(input_dim=d, output_dim=n_classes, n_hidden_layer=hidden_layers, batch_size=batch_size) #, seed=seed_weights\n",
    "\n",
    "print(\"Number of layers:\", len(model.network))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Training model......\n",
      "------------------------------------------------------\n",
      "Epoch 1:\n",
      "Train: 0.10686 Test: 0.1106 Duration: 0:00:26.969067\n",
      "------------------------------------------------------\n",
      "Epoch 2:\n",
      "Train: 0.11186 Test: 0.1238 Duration: 0:00:26.813000\n",
      "------------------------------------------------------\n",
      "Epoch 3:\n",
      "Train: 0.1159 Test: 0.1281 Duration: 0:00:26.787624\n",
      "------------------------------------------------------\n",
      "Epoch 4:\n",
      "Train: 0.11896 Test: 0.144 Duration: 0:00:26.814501\n",
      "------------------------------------------------------\n",
      "Epoch 5:\n",
      "Train: 0.12152 Test: 0.1468 Duration: 0:00:27.119508\n",
      "------------------------------------------------------\n",
      "Epoch 6:\n",
      "Train: 0.12384 Test: 0.1497 Duration: 0:00:27.118000\n",
      "------------------------------------------------------\n",
      "Epoch 7:\n",
      "Train: 0.12518 Test: 0.1521 Duration: 0:00:27.053495\n",
      "------------------------------------------------------\n",
      "Epoch 8:\n",
      "Train: 0.12836 Test: 0.1566 Duration: 0:00:27.127502\n",
      "------------------------------------------------------\n",
      "Epoch 9:\n",
      "Train: 0.13374 Test: 0.1633 Duration: 0:00:27.351000\n",
      "------------------------------------------------------\n",
      "Epoch 10:\n",
      "Train: 0.1349 Test: 0.1682 Duration: 0:00:27.051000\n",
      "------------------------------------------------------\n",
      "Epoch 11:\n",
      "Train: 0.13722 Test: 0.1738 Duration: 0:00:27.029502\n",
      "------------------------------------------------------\n",
      "Epoch 12:\n",
      "Train: 0.14008 Test: 0.1775 Duration: 0:00:27.059000\n",
      "------------------------------------------------------\n",
      "Epoch 13:\n",
      "Train: 0.14138 Test: 0.1806 Duration: 0:00:27.242998\n",
      "------------------------------------------------------\n",
      "Epoch 14:\n",
      "Train: 0.1418 Test: 0.1806 Duration: 0:00:27.593635\n",
      "------------------------------------------------------\n",
      "Epoch 15:\n",
      "Train: 0.14286 Test: 0.1834 Duration: 0:00:27.243525\n",
      "------------------------------------------------------\n",
      "Epoch 16:\n",
      "Train: 0.1434 Test: 0.1852 Duration: 0:00:27.038526\n",
      "------------------------------------------------------\n",
      "Epoch 17:\n",
      "Train: 0.14568 Test: 0.1858 Duration: 0:00:27.603499\n",
      "------------------------------------------------------\n",
      "Epoch 18:\n",
      "Train: 0.14588 Test: 0.1886 Duration: 0:00:27.592525\n",
      "------------------------------------------------------\n",
      "Epoch 19:\n",
      "Train: 0.14638 Test: 0.1889 Duration: 0:00:27.719501\n",
      "------------------------------------------------------\n",
      "Epoch 20:\n",
      "Train: 0.1468 Test: 0.1882 Duration: 0:00:27.677500\n",
      "------------------------------------------------------\n",
      "Epoch 21:\n",
      "Train: 0.14686 Test: 0.1889 Duration: 0:00:27.314499\n",
      "------------------------------------------------------\n",
      "Epoch 22:\n",
      "Train: 0.14676 Test: 0.1894 Duration: 0:00:27.083499\n",
      "------------------------------------------------------\n",
      "Epoch 23:\n",
      "Train: 0.14646 Test: 0.1894 Duration: 0:00:27.053499\n",
      "------------------------------------------------------\n",
      "Epoch 24:\n",
      "Train: 0.1465 Test: 0.1896 Duration: 0:00:27.019500\n",
      "------------------------------------------------------\n",
      "Epoch 25:\n",
      "Train: 0.14664 Test: 0.1899 Duration: 0:00:27.050501\n",
      "------------------------------------------------------\n",
      "Epoch 26:\n",
      "Train: 0.1466 Test: 0.1893 Duration: 0:00:27.196000\n",
      "------------------------------------------------------\n",
      "Epoch 27:\n",
      "Train: 0.14662 Test: 0.1891 Duration: 0:00:27.123999\n",
      "------------------------------------------------------\n",
      "Epoch 28:\n",
      "Train: 0.14668 Test: 0.1892 Duration: 0:00:27.205000\n",
      "------------------------------------------------------\n",
      "Epoch 29:\n",
      "Train: 0.14698 Test: 0.1903 Duration: 0:00:27.167501\n",
      "------------------------------------------------------\n",
      "Epoch 30:\n",
      "Train: 0.1473 Test: 0.1896 Duration: 0:00:27.128501\n",
      "------------------------------------------------------\n",
      "Epoch 31:\n",
      "Train: 0.14752 Test: 0.1897 Duration: 0:00:27.246000\n",
      "------------------------------------------------------\n",
      "Epoch 32:\n",
      "Train: 0.1477 Test: 0.1896 Duration: 0:00:27.384502\n",
      "------------------------------------------------------\n",
      "Epoch 33:\n",
      "Train: 0.1479 Test: 0.19 Duration: 0:00:27.145000\n",
      "------------------------------------------------------\n",
      "Epoch 34:\n",
      "Train: 0.14804 Test: 0.1907 Duration: 0:00:27.054500\n",
      "------------------------------------------------------\n",
      "Epoch 35:\n",
      "Train: 0.14828 Test: 0.1904 Duration: 0:00:27.152500\n",
      "------------------------------------------------------\n",
      "Epoch 36:\n",
      "Train: 0.14822 Test: 0.191 Duration: 0:00:27.388500\n",
      "------------------------------------------------------\n",
      "Epoch 37:\n",
      "Train: 0.14836 Test: 0.1907 Duration: 0:00:27.113000\n",
      "------------------------------------------------------\n",
      "Epoch 38:\n",
      "Train: 0.14856 Test: 0.1906 Duration: 0:00:27.290000\n",
      "------------------------------------------------------\n",
      "Epoch 39:\n",
      "Train: 0.14838 Test: 0.1906 Duration: 0:00:27.153000\n",
      "------------------------------------------------------\n",
      "Epoch 40:\n",
      "Train: 0.14854 Test: 0.191 Duration: 0:00:27.335001\n",
      "------------------------------------------------------\n",
      "Epoch 41:\n",
      "Train: 0.14888 Test: 0.1906 Duration: 0:00:27.121500\n",
      "------------------------------------------------------\n",
      "Epoch 42:\n",
      "Train: 0.1488 Test: 0.1908 Duration: 0:00:27.095999\n",
      "------------------------------------------------------\n",
      "Epoch 43:\n",
      "Train: 0.14904 Test: 0.1911 Duration: 0:00:27.193999\n",
      "------------------------------------------------------\n",
      "Epoch 44:\n",
      "Train: 0.14914 Test: 0.1904 Duration: 0:00:27.220000\n",
      "------------------------------------------------------\n",
      "Epoch 45:\n",
      "Train: 0.1492 Test: 0.1904 Duration: 0:00:27.045000\n",
      "------------------------------------------------------\n",
      "Epoch 46:\n",
      "Train: 0.14942 Test: 0.1912 Duration: 0:00:27.317499\n",
      "------------------------------------------------------\n",
      "Epoch 47:\n",
      "Train: 0.14972 Test: 0.1914 Duration: 0:00:27.317001\n",
      "------------------------------------------------------\n",
      "Epoch 48:\n",
      "Train: 0.14972 Test: 0.1916 Duration: 0:00:27.925002\n",
      "------------------------------------------------------\n",
      "Epoch 49:\n",
      "Train: 0.15012 Test: 0.1913 Duration: 0:00:28.474500\n",
      "------------------------------------------------------\n",
      "Epoch 50:\n",
      "Train: 0.15004 Test: 0.191 Duration: 0:00:27.979499\n"
     ]
    }
   ],
   "source": [
    "model1 = model.train(X_train, y_train, lr=lr, n_epochs=n_epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"------------------------------------------------------\")\n",
    "# print(\"Training model......\")\n",
    "\n",
    "# # Make predictions for training and test data\n",
    "# ypred_train = model.predict(X_train)\n",
    "# ypred_test = model.predict(X_test)\n",
    "# print()\n",
    "# print(\"------------------------------------------------------\")\n",
    "# print(\"Calculating accuracies.....\")\n",
    "# acc_train = accuracy(y_train, ypred_train)\n",
    "# acc_test = accuracy(y_test, ypred_test)\n",
    "# print(acc_train)\n",
    "# print(acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
