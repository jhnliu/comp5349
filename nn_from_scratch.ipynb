{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "dca0ade3e726a953b501b15e8e990130d2b7799f14cfd9f4271676035ebe5511"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Assignment 1\n",
    "## Still need to implement momentum and mini-batch SGD"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(50000, 128)\n(10000, 128)\n"
     ]
    }
   ],
   "source": [
    "# Load in the training set\n",
    "X_train = np.load('Assignment1-Dataset/train_data.npy')\n",
    "y_train = np.load('Assignment1-Dataset/train_label.npy')\n",
    "\n",
    "# # Load in the test set\n",
    "X_test = np.load('Assignment1-Dataset/test_data.npy')\n",
    "y_test = np.load('Assignment1-Dataset/test_label.npy')\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "# Normalize (optional)\n",
    "def normalize(X):\n",
    "    X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "    return X\n",
    "X_train = normalize(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly permute [0,N] and extract indices for each fold\n",
    "def crossval_folds(N, n_folds, seed=1):\n",
    "    np.random.seed(seed)\n",
    "    idx_all_permute = np.random.permutation(N)\n",
    "    N_fold = int(N/n_folds)\n",
    "    idx_folds = []\n",
    "    for i in range(n_folds):\n",
    "        start = i*N_fold\n",
    "        end = min([(i+1)*N_fold, N])\n",
    "        idx_folds.append(idx_all_permute[start:end])\n",
    "    return idx_folds"
   ]
  },
  {
   "source": [
    "## Define evaluation function"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "def eval():\n",
    "\n",
    "    # Set up\n",
    "    hidden_layers = [5] # number of nodes in hidden layers i.e. [layer1, layer2, ...]\n",
    "    lr = 0.05 # learning rate\n",
    "    n_epochs = 20 # number of training epoch\n",
    "    batch_size = 256\n",
    "    N, d = X_train.shape\n",
    "    n_classes = len(np.unique(y_train))\n",
    "\n",
    "    print(\" Data description --->  X.shape = {}, y.shape = {}, n_classes = {}\\n\".format(X_train.shape, y_train.shape, n_classes))\n",
    "    print(\"Model details:\")\n",
    "    print(\" input_dim = {}\".format(d))\n",
    "    print(\" hidden_layers = {}\".format(hidden_layers))\n",
    "    print(\" output_dim = {}\".format(n_classes))\n",
    "    print(\" eta = {}\".format(lr))\n",
    "    print(\" n_epochs = {}\".format(n_epochs))\n",
    "\n",
    "    # Train/evaluate the model on each fold\n",
    "    acc_train, acc_val = list(), list()  # training/test accuracy score\n",
    "\n",
    "    print(\"Training model......\")\n",
    "    # Build neural network classifier model and train\n",
    "    model = NN(input_dim=d, output_dim=n_classes, n_hidden_layer=hidden_layers, batch_size=batch_size) #, seed=seed_weights\n",
    "    model.train(X_train, y_train, lr=lr, n_epochs=n_epochs)\n",
    "\n",
    "    # Make predictions for training and test data\n",
    "    ypred_train = model.predict(X_train)\n",
    "    ypred_test = model.predict(X_test)\n",
    "\n",
    "    print(\"ypred_train\", len(ypred_train))\n",
    "    print(type(ypred_train))\n",
    "    print(ypred_train[:20])\n",
    "    print(\"quantity\")\n",
    "    print(type(y_train))\n",
    "    print(len(y_train))\n",
    "    # print(\"np.sum\", np.sum(y_train==ypred_train))\n",
    "    print()\n",
    "    # Compute training/test accuracy score from predicted values\n",
    "    print(\"Calculating accuracies.....\")\n",
    "    acc_train = accuracy(y_train, ypred_train)\n",
    "    acc_test = accuracy(y_test, ypred_test)\n",
    "    # acc_train = np.sum(y_train==ypred_train)/len(y_train)\n",
    "    # acc_test = np.sum(y_test==ypred_test)/len(y_test)\n",
    "\n",
    "    print(acc_train)\n",
    "    print(acc_test)\n",
    "\n",
    "def accuracy(x, y):\n",
    "    count = 0\n",
    "    assert len(x) == len(y)\n",
    "    for i in range(len(x)):\n",
    "        if x[i] == y[i]: count += 1\n",
    "    acc = count / len(x)\n",
    "    return acc"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 48,
   "outputs": []
  },
  {
   "source": [
    "## Coding the Neural Network"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN:\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, n_hidden_layer, batch_size):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.n_hidden_layer = n_hidden_layer\n",
    "        self.network = self._build_network()\n",
    "\n",
    "    # < ---- Basic numpy functions ---- > #\n",
    "    # Sigmoid (activation function)\n",
    "    def _sigmoid(self, x):\n",
    "        return 1.0/(1.0+math.exp(-x))\n",
    "\n",
    "    # Sigmoid derivative\n",
    "    def _sigmoid_derivative(self, sigmoid):\n",
    "        return sigmoid*(1.0-sigmoid)\n",
    "\n",
    "    # One-hot encoding\n",
    "    def _one_hot_encoding(self, idx, output_dim):\n",
    "        x = np.zeros(output_dim, dtype=np.int)\n",
    "        x[idx] = 1\n",
    "        return x\n",
    "\n",
    "    # ReLu activation function\n",
    "    def _relu(self, x):\n",
    "        return max(0, x)\n",
    "\n",
    "    # ReLu derivative\n",
    "    def _relu_derivative(self, x):\n",
    "        if x < 0: return 0\n",
    "        else: return 1\n",
    "    \n",
    "    # < ---- Construct the Batch Normalization Layer ---- >\n",
    "    def batch_normalize(self, x):\n",
    "        norm = np.linalg.norm(x)\n",
    "        if norm == 0: return x\n",
    "        else: return x / norm\n",
    "\n",
    "    def drop_out(self, x, rate):\n",
    "        set_0 = round(len(x) * rate)\n",
    "        for i in random.sample(range(len(x)), set_0):\n",
    "            x[i] = 0\n",
    "        return x\n",
    "\n",
    "    # < ---- Building the network architecture ---- > #\n",
    "    def _build_network(self):\n",
    "        \n",
    "        # Create a single fuly connected layer\n",
    "        def fc_layers(input_dim, output_dim):\n",
    "            layer = []\n",
    "            for i in range(output_dim): # Add a weight between each node and unit\n",
    "                weights= [random.random() for _ in range(input_dim)] # Determine FC layer with randomised/normalized w initialization\n",
    "                node = {\"weight\" : weights, \n",
    "                         \"output\": None,\n",
    "                         \"delta\": None}\n",
    "                layer.append(node) # Create the layer\n",
    "            return layer\n",
    "\n",
    "        network = [] # Build the network layer by layer\n",
    "        if len(self.n_hidden_layer) == 0:\n",
    "            network.append(fc_layers(self.input_dim, self.output_dim))\n",
    "        else:\n",
    "            network.append(fc_layers(self.input_dim, self.n_hidden_layer[0]))\n",
    "            for i in range(1, len(self.n_hidden_layer)):\n",
    "                network.append(fc_layers(self.n_hidden_layer[i-1], self.n_hidden_layer[i]))\n",
    "            network.append(fc_layers(self.n_hidden_layer[-1], self.output_dim))\n",
    "\n",
    "        return network\n",
    "\n",
    "    # < ---- Training the model ---- > #\n",
    "\n",
    "    # Training the network\n",
    "    def train(self, X, y, n_epochs=100, lr=0.005, batch_size=256):\n",
    "        for epoch in range(n_epochs):\n",
    "            print(\"Training {} epoch\".format(epoch))\n",
    "            # ============================================= #\n",
    "            # < ---- Add in mini-batch training here ---- > #\n",
    "            # ============================================= #\n",
    "            n_batches = round(len(y)/batch_size)\n",
    "            for i in range(n_batches):\n",
    "                for _X, _y, in zip(X[i*batch_size:(i+1)*batch_size], y[i*batch_size:(i+1)*batch_size]):\n",
    "                    y_label = self._one_hot_encoding(_y, self.output_dim)\n",
    "                    self._forward_pass(_X)\n",
    "                    self._back_propagation(y_label)\n",
    "                    self._update_weights(_X, lr)\n",
    "\n",
    "    # Forward-pass function\n",
    "    def _forward_pass(self, x):\n",
    "        relu = self._relu\n",
    "        transfer = self._sigmoid\n",
    "        x_in = x\n",
    "        for layer in self.network[:-1]:\n",
    "            x_out = []\n",
    "            for node in layer:\n",
    "                node['output'] = relu(np.dot(node['weight'], x_in)) # Get inner product of the input and weights\n",
    "                # node['output'] = transfer(self._dotprod(node['weight'], x_in)) # Get inner product of the input and weights\n",
    "                x_out.append(node['output'])\n",
    "            x_in = x_out # Pass the output of this layer as the input to the next layer\\\n",
    "        x_out = []\n",
    "        for node in self.network[-1]:\n",
    "            node['output'] = transfer(np.dot(node['weight'], x_in))\n",
    "            x_out.append(node['output'])\n",
    "        x_in = x_out # Pass the output of this layer as the input to the next layer\\\n",
    "\n",
    "        return x_in\n",
    "\n",
    "\n",
    "    # Back propagation function\n",
    "    def _back_propagation(self, y_label):\n",
    "        transfer_derivative = self._sigmoid_derivative\n",
    "        n_layers = len(self.network)\n",
    "        for i in reversed(range(n_layers)):\n",
    "            # Backpropagate from the output later\n",
    "            if i == n_layers - 1:\n",
    "                for j, node in enumerate(self.network[i]):\n",
    "                    err = node['output'] - y_label[j]\n",
    "                    node['delta'] = err * transfer_derivative(node['output'])\n",
    "            else:\n",
    "                # Weighted sum of gradient from upper layer\n",
    "                for j, node in enumerate(self.network[i]):\n",
    "                    err = sum([node_['weight'][j] * node_['delta'] for node_ in self.network[i+1]])\n",
    "                    node['delta'] = err * self._relu_derivative(node['output'])\n",
    "\n",
    "    def _update_weights(self, x, lr):\n",
    "        for i, layer in enumerate(self.network):\n",
    "            if i == 0:\n",
    "                inputs = x\n",
    "            else:\n",
    "                inputs = [node_['output'] for node_ in self.network[i-1]]\n",
    "\n",
    "                # Update weights\n",
    "                for node in layer:\n",
    "                    for j, inpt in enumerate(inputs):\n",
    "                        node['weight'][j] -= node['delta'] * lr * inpt\n",
    "\n",
    "    # < ---- Making predictions ---- #\n",
    "    def predict(self, x):\n",
    "        pred = np.array([np.argmax(self._forward_pass(_x)) for _x in x], dtype=np.int)\n",
    "        return pred\n",
    "\n",
    "    # < ---- Define a new optimizer here ---- > #\n",
    "    def optimizer():\n",
    "        pass"
   ]
  },
  {
   "source": [
    "## Evaluating the model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " Data description --->  X.shape = (50000, 128), y.shape = (50000, 1), n_classes = 10\n",
      "\n",
      "Model details:\n",
      " input_dim = 128\n",
      " hidden_layers = [5]\n",
      " output_dim = 10\n",
      " eta = 0.05\n",
      " n_epochs = 20\n",
      "Training model......\n",
      "Training 0 epoch\n",
      "Training 1 epoch\n",
      "Training 2 epoch\n",
      "Training 3 epoch\n",
      "Training 4 epoch\n",
      "Training 5 epoch\n",
      "Training 6 epoch\n",
      "Training 7 epoch\n",
      "Training 8 epoch\n",
      "Training 9 epoch\n",
      "Training 10 epoch\n",
      "Training 11 epoch\n",
      "Training 12 epoch\n",
      "Training 13 epoch\n",
      "Training 14 epoch\n",
      "Training 15 epoch\n",
      "Training 16 epoch\n",
      "Training 17 epoch\n",
      "Training 18 epoch\n",
      "Training 19 epoch\n",
      "ypred_train 50000\n",
      "<class 'numpy.ndarray'>\n",
      "[2 6 1 2 7 3 6 6 6 6 1 6 6 6 6 6 6 6 6 1]\n",
      "quantity\n",
      "<class 'numpy.ndarray'>\n",
      "50000\n",
      "\n",
      "Calculating accuracies.....\n",
      "0.11668\n",
      "0.1228\n"
     ]
    }
   ],
   "source": [
    "eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}